{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adamg\\AppData\\Local\\Temp\\ipykernel_22272\\666854454.py:2: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as stats\n",
    "import re\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IR Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "relevant_docs = pd.read_csv('qrels.csv')\n",
    "system_results = pd.read_csv('ttdssystemresults.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision(retrieved, relevant):\n",
    "    retrieved_set = set(retrieved)\n",
    "    relevant_set = set(relevant)\n",
    "    return len(retrieved_set.intersection(relevant_set)) / len(retrieved_set)\n",
    "\n",
    "def recall(retrieved, relevant):\n",
    "    retrieved_set = set(retrieved)\n",
    "    relevant_set = set(relevant)\n",
    "    return len(retrieved_set.intersection(relevant_set)) / len(relevant_set)\n",
    "\n",
    "def average_precision(retrieved, relevant):\n",
    "    precision_sum = 0\n",
    "    for i in range(1, len(retrieved) + 1):\n",
    "        if retrieved[i - 1] in relevant:\n",
    "            precision_sum += precision(retrieved[:i], relevant)\n",
    "    return precision_sum / len(relevant)\n",
    "\n",
    "def nDCG(retrieved, relevant, relevance_scores):\n",
    "    scores = {}\n",
    "    for i in range(len(relevant)):\n",
    "        scores[relevant[i]] = relevance_scores[i]\n",
    "    retrieved_scores = []\n",
    "    for doc in retrieved:\n",
    "        if doc in scores:\n",
    "            retrieved_scores.append(scores[doc])\n",
    "        else:\n",
    "            retrieved_scores.append(0)\n",
    "\n",
    "    DCG = retrieved_scores[0]\n",
    "    for i in range(1, len(retrieved_scores)):\n",
    "        DCG += retrieved_scores[i] / np.log2(i + 1)\n",
    "    retrieved_scores.sort(reverse=True)\n",
    "    IDCG = retrieved_scores[0]\n",
    "    for i in range(1, len(retrieved_scores)):\n",
    "        IDCG += retrieved_scores[i] / np.log2(i + 1)\n",
    "    if IDCG == 0:\n",
    "        return 0\n",
    "    return DCG / IDCG\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_ids = system_results['query_number'].unique()\n",
    "systems = system_results['system_number'].unique()\n",
    "\n",
    "results = pd.DataFrame(columns=['system_number', 'query_number', 'P@10', 'R@50', 'r-precision', 'AP', 'nDCG@10', 'nDCG@20'])\n",
    "\n",
    "for system in systems:\n",
    "    precision_10s = []\n",
    "    recall_50s = []\n",
    "    r_precisions = []\n",
    "    aps = []\n",
    "    nDCG_10s = []\n",
    "    nDCG_20s = []\n",
    "    for query_id in query_ids:\n",
    "        relevant = relevant_docs[relevant_docs['query_id'] == query_id]['doc_id'].values\n",
    "        relevance_scores = relevant_docs[relevant_docs['query_id'] == query_id]['relevance'].values\n",
    "        retrieved = system_results[(system_results['query_number'] == query_id) & (system_results['system_number'] == system)].sort_values(by='rank_of_doc')['doc_number'].values\n",
    "\n",
    "        precision_10 = precision(retrieved[:10], relevant)\n",
    "        recall_50 = recall(retrieved[:50], relevant)\n",
    "        r_precision = precision(retrieved[:len(relevant)], relevant)\n",
    "        ap = average_precision(retrieved, relevant)\n",
    "        nDCG_10 = nDCG(retrieved[:10], relevant, relevance_scores)\n",
    "        nDCG_20 = nDCG(retrieved[:20], relevant, relevance_scores)\n",
    "        \n",
    "        results.loc[len(results)] = [int(system), int(query_id), precision_10, recall_50, r_precision, ap, nDCG_10, nDCG_20]\n",
    "\n",
    "        precision_10s.append(precision_10)\n",
    "        recall_50s.append(recall_50)\n",
    "        r_precisions.append(r_precision)\n",
    "        aps.append(ap)\n",
    "        nDCG_10s.append(nDCG_10)\n",
    "        nDCG_20s.append(nDCG_20)\n",
    "\n",
    "        \n",
    "    results.loc[len(results)] = [int(system), 'mean', np.mean(precision_10s), np.mean(recall_50s), np.mean(r_precisions), np.mean(aps), np.mean(nDCG_10s), np.mean(nDCG_20s)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "results['system_number'] = results['system_number'].astype(int)\n",
    "results['query_number'] = results['query_number'].apply(lambda x: 'mean' if x == 'mean' else int(x))\n",
    "results = results.round(3)\n",
    "save_results = results.to_csv('ir_eval.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Model for P@10 : 3\n",
      "3 vs 5\n",
      "0.41 vs 0.41\n",
      "t-statistic: 0.0\n",
      "p-value: 1.0\n",
      "\n",
      "Best Model for R@50 : 2\n",
      "2 vs 1\n",
      "0.867 vs 0.834\n",
      "t-statistic: 0.46322462441985235\n",
      "p-value: 0.6442304510971641\n",
      "\n",
      "Best Model for r-precision : 3\n",
      "3 vs 6\n",
      "0.448 vs 0.448\n",
      "t-statistic: 0.0\n",
      "p-value: 1.0\n",
      "\n",
      "Best Model for AP : 3\n",
      "3 vs 6\n",
      "0.451 vs 0.445\n",
      "t-statistic: 0.060328233925037125\n",
      "p-value: 0.952017072315338\n",
      "\n",
      "Best Model for nDCG@10 : 3\n",
      "3 vs 6\n",
      "0.592 vs 0.571\n",
      "t-statistic: 0.21289482573886245\n",
      "p-value: 0.8318512285899999\n",
      "\n",
      "Best Model for nDCG@20 : 3\n",
      "3 vs 1\n",
      "0.584 vs 0.566\n",
      "t-statistic: 0.18209000555013286\n",
      "p-value: 0.8558882168842699\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mean_results = results[results['query_number'] == 'mean']\n",
    "for column in ['P@10', 'R@50', 'r-precision', 'AP', 'nDCG@10', 'nDCG@20']:\n",
    "    df = mean_results[['system_number', column]]\n",
    "    df = df.set_index('system_number')\n",
    "    df.sort_values(by=column, ascending=False, inplace=True)\n",
    "    print(\"Best Model for\", column, \":\", df.index[0])\n",
    "\n",
    "    best_model = df.index[0]\n",
    "    second_best_model = df.index[1]\n",
    "    best_model_value = df.loc[best_model][column]\n",
    "    second_best_model_value = df.loc[second_best_model][column]\n",
    "    # two tailed t-test\n",
    "    print(best_model, \"vs\", second_best_model)\n",
    "    print(best_model_value, \"vs\", second_best_model_value)\n",
    "    t_statistic = (best_model_value - second_best_model_value) / np.sqrt((best_model_value * (1 - best_model_value) / 50) + (second_best_model_value * (1 - second_best_model_value) / 50))\n",
    "    print(\"t-statistic:\", t_statistic)\n",
    "    print(\"p-value:\", 2 * (1 - stats.t.cdf(t_statistic, 98)))\n",
    "    print(\"\")\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "bible_and_quran = pd.read_csv(\"bible_and_quran.tsv\", sep='\\t', header=None)\n",
    "bible_and_quran.columns = ['Source', 'Text']\n",
    "ot, nt, quran = bible_and_quran[bible_and_quran['Source'] == 'OT'], bible_and_quran[bible_and_quran['Source'] == 'NT'], bible_and_quran[bible_and_quran['Source'] == 'Quran']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    \"\"\"\n",
    "    Tokenize the text and return a list of words.\n",
    "    Tokenisation is done by splitting the text, making it lowercase, and replacing any non-alphanumeric characters with spaces.\n",
    "    :param text: The text to be tokenized\n",
    "    :return: A list of words\n",
    "    \"\"\"\n",
    "    tokens = text.split()\n",
    "    words = [re.sub(r'[^a-zA-Z0-9]', ' ', token) for token in tokens]\n",
    "    words = [word.lower() for word in words if word != '' or word != ' ']\n",
    "    words = [word.strip() for word in words]\n",
    "    return words\n",
    "\n",
    "def remove_stopwords(words, stop_words_file=\"stop_words.txt\"):\n",
    "    \"\"\"\n",
    "    Remove stopwords from the list of words and return the filtered list.\n",
    "    Stopwords are read from the file specified in the stop_words_file parameter.\n",
    "    :param words: The list of words\n",
    "    :param stop_words_file: The path to the file containing the stopwords\n",
    "    :return: The filtered list of words\n",
    "    \"\"\"\n",
    "    with open(stop_words_file, 'r') as f:\n",
    "        stop_words = f.readlines()\n",
    "    stop_words = [word.strip() for word in stop_words]\n",
    "    filtered_words = [word for word in words if word not in stop_words]\n",
    "    return filtered_words\n",
    "\n",
    "def stem(words):\n",
    "    \"\"\"\n",
    "    Stem the words using the Porter stemmer and return the list of stemmed words.\n",
    "    :param words: The list of words\n",
    "    :return: The list of stemmed words\n",
    "    \"\"\"\n",
    "    stemmer = nltk.stem.PorterStemmer()\n",
    "    return [stemmer.stem(word) for word in words]\n",
    "\n",
    "def preprocess(text):\n",
    "    \"\"\"\n",
    "    Preprocess the text by tokenizing, removing stopwords, and stemming the words.\n",
    "    :param text: The text to be preprocessed\n",
    "    :param stopping: A boolean value indicating whether to remove stopwords or not\n",
    "    :return: The list of preprocessed words\n",
    "    \"\"\"\n",
    "    words = tokenize(text)\n",
    "    words = remove_stopwords(words)\n",
    "    words = stem(words)\n",
    "    return words\n",
    "\n",
    "def preprocess_corpus(corpus):\n",
    "    \"\"\"\n",
    "    Preprocess the entire corpus by tokenizing, removing stopwords, and stemming the words.\n",
    "    :param corpus: The corpus to be preprocessed\n",
    "    :param stopping: A boolean value indicating whether to remove stopwords or not\n",
    "    :return: The list of preprocessed words\n",
    "    \"\"\"\n",
    "    preprocessed_corpus = []\n",
    "    for doc in corpus['Text']:\n",
    "        words = preprocess(doc)\n",
    "        preprocessed_corpus.append(words)\n",
    "    return preprocessed_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "ot_tokens = preprocess_corpus(ot)\n",
    "nt_tokens = preprocess_corpus(nt)\n",
    "quran_tokens = preprocess_corpus(quran)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique Tokens: 7988\n",
      "1000 tokens processed\n",
      "2000 tokens processed\n",
      "3000 tokens processed\n",
      "4000 tokens processed\n",
      "5000 tokens processed\n",
      "6000 tokens processed\n",
      "7000 tokens processed\n"
     ]
    }
   ],
   "source": [
    "# Compute Mutual Information\n",
    "\n",
    "def mutual_information_and_chi_squared(word, corpus, other_corpuses):\n",
    "    \"\"\"\n",
    "    Compute the mutual information of a word in two corpora.\n",
    "    :param word: The word for which to compute the mutual information\n",
    "    :param corpus1: The first corpus\n",
    "    :param corpus2: The second corpus\n",
    "    :return: The mutual information of the word in the two corpora\n",
    "    \"\"\"\n",
    "\n",
    "    N_11 = 0\n",
    "    N_10 = 0\n",
    "    N_01 = 0\n",
    "    N_00 = 0\n",
    "\n",
    "    for doc in corpus:\n",
    "        if word in doc:\n",
    "            N_11 += 1\n",
    "        else:\n",
    "            N_01 += 1\n",
    "    for doc in other_corpuses:\n",
    "        if word in doc:\n",
    "            N_10 += 1\n",
    "        else:\n",
    "            N_00 += 1\n",
    "\n",
    "    N = N_11 + N_10 + N_01 + N_00\n",
    "    if N_00 == 0 or N_01 == 0 or N_10 == 0 or N_11 == 0:\n",
    "        return 0, 0\n",
    "\n",
    "    mi = N_11 / N * np.log2(N * N_11 / ((N_10 + N_11) * (N_01 + N_11))) + \\\n",
    "        N_01 / N * np.log2(N * N_01 / ((N_00 + N_01) * (N_01 + N_11))) + \\\n",
    "        N_10 / N * np.log2(N * N_10 / ((N_10 + N_11) * (N_00 + N_10))) + \\\n",
    "        N_00 / N * np.log2(N * N_00 / ((N_00 + N_01) * (N_00 + N_10)))\n",
    "    \n",
    "    chi_squared = N * (N_11 * N_00 - N_10 * N_01) ** 2 / ((N_11 + N_01) * (N_11 + N_10) * (N_10 + N_00) * (N_01 + N_00))\n",
    "    \n",
    "    return mi, chi_squared\n",
    "\n",
    "def top_mutual_information_chi_squared(corpus, other_corpuses):\n",
    "    \"\"\"\n",
    "    Compute the top n words with the highest mutual information in two corpora.\n",
    "    :param corpus1: The first corpus\n",
    "    :param corpus2: The second corpus\n",
    "    :param n: The number of words to return\n",
    "    :return: A df containing the words and their mutual information and chi-squared values\n",
    "    \"\"\"\n",
    "    df = pd.DataFrame(columns=['Word', 'Mutual Information', 'Chi-Squared'])\n",
    "    words = set()\n",
    "    for doc in corpus:\n",
    "        new_words = set(doc)\n",
    "        words = words.union(new_words)\n",
    "    # for doc in other_corpuses:\n",
    "    #     new_words = set(doc)\n",
    "    #     words = words.union(new_words)\n",
    "    print(\"Unique Tokens: \" + str(len(words)))\n",
    "    i = 0\n",
    "    for word in words:\n",
    "        mi, chi_squared = mutual_information_and_chi_squared(word, corpus, other_corpuses)\n",
    "        df.loc[len(df)] = [word, mi, chi_squared]\n",
    "        i += 1\n",
    "        if i % 1000 == 0:\n",
    "            print(str(i) + \" tokens processed\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Modelling (LDA with Gibbs Sampling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora\n",
    "from gensim.models import LdaModel\n",
    "\n",
    "text = ot_tokens + nt_tokens + quran_tokens\n",
    "\n",
    "dictionary = corpora.Dictionary(text)\n",
    "corpus = [dictionary.doc2bow(doc) for doc in text]\n",
    "\n",
    "lda_model = LdaModel(corpus, num_topics=20, id2word=dictionary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each corpus, compute the average score for each topic by summing the document-topic probability for each document in that corpus and dividing by the total number of documents in the corpus. \n",
    "\n",
    "corpuses = [ot_tokens, nt_tokens, quran_tokens]\n",
    "corpus_names = ['OT', 'NT', 'Quran']\n",
    "corpus_topics = dict()\n",
    "for i in range(len(corpuses)):\n",
    "    corpus = corpuses[i]\n",
    "    corpus_name = corpus_names[i]\n",
    "    corpus_topic = np.zeros(20)\n",
    "    for doc in corpus:\n",
    "        bow = dictionary.doc2bow(doc)\n",
    "        doc_topics = lda_model.get_document_topics(bow)\n",
    "        for topic in doc_topics:\n",
    "            corpus_topic[topic[0]] += topic[1]\n",
    "    corpus_topic /= len(corpus)\n",
    "    corpus_topics[corpus_name] = corpus_topic\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 3 topics for OT\n",
      "Topic 0 : 0.103*\"god\" + 0.080*\"lord\" + 0.049*\"truth\" + 0.044*\"word\" + 0.040*\"suffer\" + 0.029*\"good\" + 0.028*\"merci\" + 0.026*\"evil\" + 0.026*\"peopl\" + 0.025*\"righteous\"\n",
      "Topic 6 : 0.074*\"god\" + 0.062*\"peopl\" + 0.051*\"lord\" + 0.026*\"guid\" + 0.024*\"honor\" + 0.018*\"dwell\" + 0.016*\"mountain\" + 0.016*\"abraham\" + 0.016*\"deni\" + 0.016*\"fear\"\n",
      "Topic 19 : 0.218*\"god\" + 0.041*\"earth\" + 0.039*\"heaven\" + 0.038*\"lord\" + 0.036*\"judgment\" + 0.027*\"love\" + 0.023*\"day\" + 0.022*\"soul\" + 0.021*\"live\" + 0.020*\"peopl\"\n",
      "\n",
      "Top 3 topics for NT\n",
      "Topic 0 : 0.103*\"god\" + 0.080*\"lord\" + 0.049*\"truth\" + 0.044*\"word\" + 0.040*\"suffer\" + 0.029*\"good\" + 0.028*\"merci\" + 0.026*\"evil\" + 0.026*\"peopl\" + 0.025*\"righteous\"\n",
      "Topic 19 : 0.218*\"god\" + 0.041*\"earth\" + 0.039*\"heaven\" + 0.038*\"lord\" + 0.036*\"judgment\" + 0.027*\"love\" + 0.023*\"day\" + 0.022*\"soul\" + 0.021*\"live\" + 0.020*\"peopl\"\n",
      "Topic 7 : 0.102*\"messeng\" + 0.047*\"power\" + 0.043*\"death\" + 0.035*\"thing\" + 0.029*\"belong\" + 0.025*\"end\" + 0.025*\"earth\" + 0.020*\"fruit\" + 0.017*\"die\" + 0.016*\"purpos\"\n",
      "\n",
      "Top 3 topics for Quran\n",
      "Topic 19 : 0.218*\"god\" + 0.041*\"earth\" + 0.039*\"heaven\" + 0.038*\"lord\" + 0.036*\"judgment\" + 0.027*\"love\" + 0.023*\"day\" + 0.022*\"soul\" + 0.021*\"live\" + 0.020*\"peopl\"\n",
      "Topic 0 : 0.103*\"god\" + 0.080*\"lord\" + 0.049*\"truth\" + 0.044*\"word\" + 0.040*\"suffer\" + 0.029*\"good\" + 0.028*\"merci\" + 0.026*\"evil\" + 0.026*\"peopl\" + 0.025*\"righteous\"\n",
      "Topic 6 : 0.074*\"god\" + 0.062*\"peopl\" + 0.051*\"lord\" + 0.026*\"guid\" + 0.024*\"honor\" + 0.018*\"dwell\" + 0.016*\"mountain\" + 0.016*\"abraham\" + 0.016*\"deni\" + 0.016*\"fear\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# top 3 topics for each corpus\n",
    "for corpus_name in corpus_names:\n",
    "    print(\"Top 3 topics for\", corpus_name)\n",
    "    topics = corpus_topics[corpus_name]\n",
    "    top_topics = np.argsort(topics)[::-1][:3]\n",
    "    for topic in top_topics:\n",
    "        print(\"Topic\", topic, \":\", lda_model.print_topic(topic))\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_file = \"train.txt\"\n",
    "\n",
    "with "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
