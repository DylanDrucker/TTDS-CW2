{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adamg\\AppData\\Local\\Temp\\ipykernel_18088\\4107834475.py:2: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as stats\n",
    "import re\n",
    "import nltk\n",
    "\n",
    "# some prereqs:\n",
    "import collections\n",
    "\n",
    "# regular expressions\n",
    "import re\n",
    "\n",
    "# for string.punctuation: list of punctuation characters\n",
    "import string\n",
    "\n",
    "import sklearn\n",
    "\n",
    "# import this for storing our BOW format\n",
    "import scipy\n",
    "from scipy import sparse\n",
    "\n",
    "# numpy for more easily storing multidimensional data\n",
    "import numpy as np\n",
    "\n",
    "# scikit learn. Contains lots of ML models we can use\n",
    "# import the library for support vector machines\n",
    "from sklearn import svm\n",
    "from sklearn import ensemble\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IR Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "relevant_docs = pd.read_csv('qrels.csv')\n",
    "system_results = pd.read_csv('ttdssystemresults.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision(retrieved, relevant):\n",
    "    retrieved_set = set(retrieved)\n",
    "    relevant_set = set(relevant)\n",
    "    return len(retrieved_set.intersection(relevant_set)) / len(retrieved_set)\n",
    "\n",
    "def recall(retrieved, relevant):\n",
    "    retrieved_set = set(retrieved)\n",
    "    relevant_set = set(relevant)\n",
    "    return len(retrieved_set.intersection(relevant_set)) / len(relevant_set)\n",
    "\n",
    "def average_precision(retrieved, relevant):\n",
    "    precision_sum = 0\n",
    "    for i in range(1, len(retrieved) + 1):\n",
    "        if retrieved[i - 1] in relevant:\n",
    "            precision_sum += precision(retrieved[:i], relevant)\n",
    "    return precision_sum / len(relevant)\n",
    "\n",
    "def nDCG(retrieved, relevant, relevance_scores):\n",
    "    scores = {}\n",
    "    for i in range(len(relevant)):\n",
    "        scores[relevant[i]] = relevance_scores[i]\n",
    "    retrieved_scores = []\n",
    "    for doc in retrieved:\n",
    "        if doc in scores:\n",
    "            retrieved_scores.append(scores[doc])\n",
    "        else:\n",
    "            retrieved_scores.append(0)\n",
    "\n",
    "    DCG = retrieved_scores[0]\n",
    "    for i in range(1, len(retrieved_scores)):\n",
    "        DCG += retrieved_scores[i] / np.log2(i + 1)\n",
    "    retrieved_scores.sort(reverse=True)\n",
    "    IDCG = retrieved_scores[0]\n",
    "    for i in range(1, len(retrieved_scores)):\n",
    "        IDCG += retrieved_scores[i] / np.log2(i + 1)\n",
    "    if IDCG == 0:\n",
    "        return 0\n",
    "    \n",
    "    print(\"DCG\", DCG)\n",
    "    print(\"IDCG\", IDCG)\n",
    "    return DCG / IDCG\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing system 1\n",
      "Query ID 1\n",
      "DCG 2.997147735133648\n",
      "IDCG 6.7618595071429155\n",
      "DCG 2.997147735133648\n",
      "IDCG 6.7618595071429155\n",
      "Query ID 2\n",
      "DCG 1.7317065537373744\n",
      "IDCG 2.6309297535714578\n",
      "DCG 3.855081630315694\n",
      "IDCG 7.268929392892205\n",
      "Query ID 3\n",
      "DCG 0.6941346394792774\n",
      "IDCG 3.0\n",
      "Query ID 4\n",
      "DCG 5.902918195508732\n",
      "IDCG 8.579388872450851\n",
      "DCG 8.995617437316119\n",
      "IDCG 12.803884063230115\n",
      "Query ID 5\n",
      "DCG 1.0177825608059992\n",
      "IDCG 2.0\n",
      "DCG 1.2575950273741305\n",
      "IDCG 2.6309297535714578\n",
      "Query ID 6\n",
      "DCG 6.508353076911336\n",
      "IDCG 8.304666305987414\n",
      "DCG 8.320363096230484\n",
      "IDCG 11.084361790882394\n",
      "Query ID 7\n",
      "DCG 3.3010299956639813\n",
      "IDCG 4.0\n",
      "DCG 3.3010299956639813\n",
      "IDCG 4.0\n",
      "Query ID 8\n",
      "DCG 6.018590298918789\n",
      "IDCG 7.57938887245085\n",
      "DCG 6.956195252178463\n",
      "IDCG 9.768929392892208\n",
      "Query ID 9\n",
      "DCG 6.6137831393950375\n",
      "IDCG 8.584394269677935\n",
      "DCG 9.910340050543956\n",
      "IDCG 13.294270125579557\n",
      "Query ID 10\n",
      "DCG 1.1821938260239127\n",
      "IDCG 5.0\n",
      "Processing system 2\n",
      "Query ID 1\n",
      "DCG 0.6666666666666666\n",
      "IDCG 2.0\n",
      "DCG 1.2245525579689263\n",
      "IDCG 4.0\n",
      "Query ID 2\n",
      "DCG 0.8306573070062113\n",
      "IDCG 2.6309297535714578\n",
      "Query ID 3\n",
      "Query ID 4\n",
      "DCG 4.254580407512063\n",
      "IDCG 5.948459118879392\n",
      "DCG 6.674972459757823\n",
      "IDCG 9.723670191738254\n",
      "Query ID 5\n",
      "DCG 0.3010299956639812\n",
      "IDCG 1.0\n",
      "DCG 0.3010299956639812\n",
      "IDCG 1.0\n",
      "Query ID 6\n",
      "DCG 2.351549105871336\n",
      "IDCG 5.130929753571458\n",
      "DCG 2.849607554275168\n",
      "IDCG 5.948459118879392\n",
      "Query ID 7\n",
      "DCG 5.2618595071429155\n",
      "IDCG 5.630929753571458\n",
      "DCG 5.2618595071429155\n",
      "IDCG 5.630929753571458\n",
      "Query ID 8\n",
      "DCG 0.9463946303571862\n",
      "IDCG 3.0\n",
      "DCG 1.1818035437238243\n",
      "IDCG 4.0\n",
      "Query ID 9\n",
      "DCG 2.8612125843351066\n",
      "IDCG 4.561606311644851\n",
      "DCG 3.6483358590568264\n",
      "IDCG 5.637999639320747\n",
      "Query ID 10\n",
      "DCG 0.3562071871080222\n",
      "IDCG 1.0\n",
      "DCG 1.0829830386511141\n",
      "IDCG 3.6309297535714578\n",
      "Processing system 3\n",
      "Query ID 1\n",
      "DCG 5.0\n",
      "IDCG 5.630929753571458\n",
      "DCG 5.557885891302259\n",
      "IDCG 6.7618595071429155\n",
      "Query ID 2\n",
      "DCG 6.050519110207355\n",
      "IDCG 6.57938887245085\n",
      "DCG 6.517306236733752\n",
      "IDCG 7.268929392892205\n",
      "Query ID 3\n",
      "DCG 0.7194373997043944\n",
      "IDCG 3.0\n",
      "Query ID 4\n",
      "DCG 8.903988123508077\n",
      "IDCG 10.399859146463665\n",
      "DCG 10.96663221060209\n",
      "IDCG 12.803884063230115\n",
      "Query ID 5\n",
      "DCG 1.0050053972270843\n",
      "IDCG 2.6309297535714578\n",
      "DCG 1.0050053972270843\n",
      "IDCG 2.6309297535714578\n",
      "Query ID 6\n",
      "DCG 1.391858204461626\n",
      "IDCG 3.1309297535714578\n",
      "DCG 4.98191114491435\n",
      "IDCG 11.084361790882394\n",
      "Query ID 7\n",
      "Query ID 8\n",
      "DCG 7.624119628005875\n",
      "IDCG 9.768929392892208\n",
      "DCG 7.624119628005875\n",
      "IDCG 9.768929392892208\n",
      "Query ID 9\n",
      "DCG 6.3338833842213065\n",
      "IDCG 9.768929392892208\n",
      "DCG 9.459994777646427\n",
      "IDCG 13.294270125579557\n",
      "Query ID 10\n",
      "DCG 3.0\n",
      "IDCG 3.0\n",
      "DCG 3.511916049619631\n",
      "IDCG 5.0\n",
      "Processing system 4\n",
      "Query ID 1\n",
      "DCG 0.4627564263195183\n",
      "IDCG 2.0\n",
      "Query ID 2\n",
      "DCG 0.6309297535714575\n",
      "IDCG 2.0\n",
      "DCG 0.6309297535714575\n",
      "IDCG 2.0\n",
      "Query ID 3\n",
      "Query ID 4\n",
      "DCG 1.4871369406794797\n",
      "IDCG 2.6309297535714578\n",
      "DCG 1.9887533082665585\n",
      "IDCG 3.5616063116448506\n",
      "Query ID 5\n",
      "DCG 1.0\n",
      "IDCG 1.0\n",
      "DCG 1.0\n",
      "IDCG 1.0\n",
      "Query ID 6\n",
      "DCG 0.43067655807339306\n",
      "IDCG 1.0\n",
      "DCG 0.43067655807339306\n",
      "IDCG 1.0\n",
      "Query ID 7\n",
      "Query ID 8\n",
      "DCG 0.2890648263178879\n",
      "IDCG 1.0\n",
      "Query ID 9\n",
      "DCG 2.9030899869919438\n",
      "IDCG 5.0\n",
      "DCG 2.9030899869919438\n",
      "IDCG 5.0\n",
      "Query ID 10\n",
      "Processing system 5\n",
      "Query ID 1\n",
      "DCG 2.3026018174652085\n",
      "IDCG 5.630929753571458\n",
      "DCG 2.8604877087674683\n",
      "IDCG 6.7618595071429155\n",
      "Query ID 2\n",
      "DCG 4.691545553974586\n",
      "IDCG 6.57938887245085\n",
      "DCG 5.158332680500983\n",
      "IDCG 7.268929392892205\n",
      "Query ID 3\n",
      "DCG 0.7194373997043944\n",
      "IDCG 3.0\n",
      "Query ID 4\n",
      "DCG 7.592544611691694\n",
      "IDCG 10.399859146463665\n",
      "DCG 9.655188698785706\n",
      "IDCG 12.803884063230115\n",
      "Query ID 5\n",
      "DCG 1.0033476796842515\n",
      "IDCG 2.6309297535714578\n",
      "DCG 1.0033476796842515\n",
      "IDCG 2.6309297535714578\n",
      "Query ID 6\n",
      "DCG 2.047171430523103\n",
      "IDCG 3.1309297535714578\n",
      "DCG 5.637224370975828\n",
      "IDCG 11.084361790882394\n",
      "Query ID 7\n",
      "Query ID 8\n",
      "DCG 7.3905155583106055\n",
      "IDCG 9.768929392892208\n",
      "DCG 7.3905155583106055\n",
      "IDCG 9.768929392892208\n",
      "Query ID 9\n",
      "DCG 6.397585444059894\n",
      "IDCG 9.768929392892208\n",
      "DCG 9.523696837485014\n",
      "IDCG 13.294270125579557\n",
      "Query ID 10\n",
      "DCG 1.0\n",
      "IDCG 3.0\n",
      "DCG 1.5119160496196309\n",
      "IDCG 5.0\n",
      "Processing system 6\n",
      "Query ID 1\n",
      "DCG 4.892789260714372\n",
      "IDCG 5.630929753571458\n",
      "DCG 5.4506751520166326\n",
      "IDCG 6.7618595071429155\n",
      "Query ID 2\n",
      "DCG 5.050519110207355\n",
      "IDCG 6.57938887245085\n",
      "DCG 5.517306236733752\n",
      "IDCG 7.268929392892205\n",
      "Query ID 3\n",
      "DCG 0.7194373997043944\n",
      "IDCG 3.0\n",
      "Query ID 4\n",
      "DCG 8.104241319006139\n",
      "IDCG 10.399859146463665\n",
      "DCG 10.166885406100151\n",
      "IDCG 12.803884063230115\n",
      "Query ID 5\n",
      "DCG 1.0050053972270843\n",
      "IDCG 2.6309297535714578\n",
      "DCG 1.0050053972270843\n",
      "IDCG 2.6309297535714578\n",
      "Query ID 6\n",
      "DCG 1.391858204461626\n",
      "IDCG 3.1309297535714578\n",
      "DCG 4.98191114491435\n",
      "IDCG 11.084361790882394\n",
      "Query ID 7\n",
      "Query ID 8\n",
      "DCG 7.054796186079268\n",
      "IDCG 9.768929392892208\n",
      "DCG 7.054796186079268\n",
      "IDCG 9.768929392892208\n",
      "Query ID 9\n",
      "DCG 7.2645599422947\n",
      "IDCG 9.768929392892208\n",
      "DCG 10.390671335719821\n",
      "IDCG 13.294270125579557\n",
      "Query ID 10\n",
      "DCG 3.0\n",
      "IDCG 3.0\n",
      "DCG 3.511916049619631\n",
      "IDCG 5.0\n"
     ]
    }
   ],
   "source": [
    "query_ids = system_results['query_number'].unique()\n",
    "systems = system_results['system_number'].unique()\n",
    "\n",
    "results = pd.DataFrame(columns=['system_number', 'query_number', 'P@10', 'R@50', 'r-precision', 'AP', 'nDCG@10', 'nDCG@20'])\n",
    "\n",
    "for system in systems:\n",
    "    print(\"Processing system\", system)\n",
    "    precision_10s = []\n",
    "    recall_50s = []\n",
    "    r_precisions = []\n",
    "    aps = []\n",
    "    nDCG_10s = []\n",
    "    nDCG_20s = []\n",
    "    for query_id in query_ids:\n",
    "        print(\"Query ID\", query_id)\n",
    "        relevant = relevant_docs[relevant_docs['query_id'] == query_id]['doc_id'].values\n",
    "        relevance_scores = relevant_docs[relevant_docs['query_id'] == query_id]['relevance'].values\n",
    "        retrieved = system_results[(system_results['query_number'] == query_id) & (system_results['system_number'] == system)].sort_values(by='rank_of_doc')['doc_number'].values\n",
    "\n",
    "        precision_10 = precision(retrieved[:10], relevant)\n",
    "        recall_50 = recall(retrieved[:50], relevant)\n",
    "        r_precision = precision(retrieved[:len(relevant)], relevant)\n",
    "        ap = average_precision(retrieved, relevant)\n",
    "        nDCG_10 = nDCG(retrieved[:10], relevant, relevance_scores)\n",
    "        nDCG_20 = nDCG(retrieved[:20], relevant, relevance_scores)\n",
    "        \n",
    "        results.loc[len(results)] = [int(system), int(query_id), precision_10, recall_50, r_precision, ap, nDCG_10, nDCG_20]\n",
    "\n",
    "        precision_10s.append(precision_10)\n",
    "        recall_50s.append(recall_50)\n",
    "        r_precisions.append(r_precision)\n",
    "        aps.append(ap)\n",
    "        nDCG_10s.append(nDCG_10)\n",
    "        nDCG_20s.append(nDCG_20)\n",
    "\n",
    "        \n",
    "    results.loc[len(results)] = [int(system), 'mean', np.mean(precision_10s), np.mean(recall_50s), np.mean(r_precisions), np.mean(aps), np.mean(nDCG_10s), np.mean(nDCG_20s)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "results['system_number'] = results['system_number'].astype(int)\n",
    "results['query_number'] = results['query_number'].apply(lambda x: 'mean' if x == 'mean' else int(x))\n",
    "results = results.round(3)\n",
    "save_results = results.to_csv('ir_eval.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Model for P@10 : 3\n",
      "3 vs 5\n",
      "0.41 vs 0.41\n",
      "t-statistic: 0.0\n",
      "p-value: 1.0\n",
      "\n",
      "Best Model for R@50 : 2\n",
      "2 vs 1\n",
      "0.867 vs 0.834\n",
      "t-statistic: 0.46322462441985235\n",
      "p-value: 0.6442304510971641\n",
      "\n",
      "Best Model for r-precision : 3\n",
      "3 vs 6\n",
      "0.448 vs 0.448\n",
      "t-statistic: 0.0\n",
      "p-value: 1.0\n",
      "\n",
      "Best Model for AP : 3\n",
      "3 vs 6\n",
      "0.451 vs 0.445\n",
      "t-statistic: 0.060328233925037125\n",
      "p-value: 0.952017072315338\n",
      "\n",
      "Best Model for nDCG@10 : 3\n",
      "3 vs 6\n",
      "0.592 vs 0.571\n",
      "t-statistic: 0.21289482573886245\n",
      "p-value: 0.8318512285899999\n",
      "\n",
      "Best Model for nDCG@20 : 3\n",
      "3 vs 1\n",
      "0.584 vs 0.566\n",
      "t-statistic: 0.18209000555013286\n",
      "p-value: 0.8558882168842699\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mean_results = results[results['query_number'] == 'mean']\n",
    "for column in ['P@10', 'R@50', 'r-precision', 'AP', 'nDCG@10', 'nDCG@20']:\n",
    "    df = mean_results[['system_number', column]]\n",
    "    df = df.set_index('system_number')\n",
    "    df.sort_values(by=column, ascending=False, inplace=True)\n",
    "    print(\"Best Model for\", column, \":\", df.index[0])\n",
    "\n",
    "    best_model = df.index[0]\n",
    "    second_best_model = df.index[1]\n",
    "    best_model_value = df.loc[best_model][column]\n",
    "    second_best_model_value = df.loc[second_best_model][column]\n",
    "    # two tailed t-test\n",
    "    print(best_model, \"vs\", second_best_model)\n",
    "    print(best_model_value, \"vs\", second_best_model_value)\n",
    "    t_statistic = (best_model_value - second_best_model_value) / np.sqrt((best_model_value * (1 - best_model_value) / 50) + (second_best_model_value * (1 - second_best_model_value) / 50))\n",
    "    print(\"t-statistic:\", t_statistic)\n",
    "    print(\"p-value:\", 2 * (1 - stats.t.cdf(t_statistic, 98)))\n",
    "    print(\"\")\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "bible_and_quran = pd.read_csv(\"bible_and_quran.tsv\", sep='\\t', header=None)\n",
    "bible_and_quran.columns = ['Source', 'Text']\n",
    "ot, nt, quran = bible_and_quran[bible_and_quran['Source'] == 'OT'], bible_and_quran[bible_and_quran['Source'] == 'NT'], bible_and_quran[bible_and_quran['Source'] == 'Quran']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def tokenize(text):\n",
    "#     \"\"\"\n",
    "#     Tokenize the text and return a list of words.\n",
    "#     Tokenisation is done by splitting the text, making it lowercase, and replacing any non-alphanumeric characters with spaces.\n",
    "#     :param text: The text to be tokenized\n",
    "#     :return: A list of words\n",
    "#     \"\"\"\n",
    "#     tokens = text.split()\n",
    "#     words = [re.sub(r'[^a-zA-Z0-9]', '', token) for token in tokens]\n",
    "#     words = [word.lower() for word in words if word != '' or word != ' ']\n",
    "#     words = [word.strip() for word in words]\n",
    "#     return words\n",
    "\n",
    "# def remove_stopwords(words, stop_words_file=\"stop_words.txt\"):\n",
    "#     \"\"\"\n",
    "#     Remove stopwords from the list of words and return the filtered list.\n",
    "#     Stopwords are read from the file specified in the stop_words_file parameter.\n",
    "#     :param words: The list of words\n",
    "#     :param stop_words_file: The path to the file containing the stopwords\n",
    "#     :return: The filtered list of words\n",
    "#     \"\"\"\n",
    "#     with open(stop_words_file, 'r') as f:\n",
    "#         stop_words = f.readlines()\n",
    "#     stop_words = [word.strip() for word in stop_words]\n",
    "#     filtered_words = [word for word in words if word not in stop_words]\n",
    "#     return filtered_words\n",
    "\n",
    "# def stem(words):\n",
    "#     \"\"\"\n",
    "#     Stem the words using the Porter stemmer and return the list of stemmed words.\n",
    "#     :param words: The list of words\n",
    "#     :return: The list of stemmed words\n",
    "#     \"\"\"\n",
    "#     stemmer = nltk.stem.PorterStemmer()\n",
    "#     return [stemmer.stem(word) for word in words]\n",
    "\n",
    "def preprocess(text, stop_words_file=\"stop_words.txt\"):\n",
    "    \"\"\"\n",
    "    Preprocess the text by tokenizing, removing stopwords, and stemming the words.\n",
    "    :param text: The text to be preprocessed\n",
    "    :param stopping: A boolean value indicating whether to remove stopwords or not\n",
    "    :return: The list of preprocessed words\n",
    "    \"\"\"\n",
    "    with open(stop_words_file, 'r') as f:\n",
    "        stop_words = f.readlines()\n",
    "    stop_words = [word.strip() for word in stop_words]\n",
    "\n",
    "    tokens = text.split()\n",
    "    tokens = [token.strip() for token in tokens]\n",
    "    tokens = [token.lower() for token in tokens]\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "    words = [re.sub(r'[^a-zA-Z0-9]', '', token) for token in tokens]\n",
    "    words = [word for word in words if word != '' or word != ' ']\n",
    "    stemmer = nltk.stem.PorterStemmer()\n",
    "    words = [stemmer.stem(word) for word in words]\n",
    "    return words\n",
    "\n",
    "def preprocess_corpus(corpus):\n",
    "    \"\"\"\n",
    "    Preprocess the entire corpus by tokenizing, removing stopwords, and stemming the words.\n",
    "    :param corpus: The corpus to be preprocessed\n",
    "    :param stopping: A boolean value indicating whether to remove stopwords or not\n",
    "    :return: The list of preprocessed words\n",
    "    \"\"\"\n",
    "    preprocessed_corpus = []\n",
    "    for doc in corpus['Text']:\n",
    "        words = preprocess(doc)\n",
    "        preprocessed_corpus.append(words)\n",
    "    return preprocessed_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "ot_tokens = preprocess_corpus(ot)\n",
    "nt_tokens = preprocess_corpus(nt)\n",
    "quran_tokens = preprocess_corpus(quran)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Mutual Information\n",
    "\n",
    "def mutual_information_and_chi_squared(word, corpus, other_corpuses):\n",
    "    \"\"\"\n",
    "    Compute the mutual information of a word in two corpora.\n",
    "    :param word: The word for which to compute the mutual information\n",
    "    :param corpus1: The first corpus\n",
    "    :param corpus2: The second corpus\n",
    "    :return: The mutual information of the word in the two corpora\n",
    "    \"\"\"\n",
    "\n",
    "    N_11 = 0\n",
    "    N_10 = 0\n",
    "    N_01 = 0\n",
    "    N_00 = 0\n",
    "\n",
    "    for doc in corpus:\n",
    "        if word in doc:\n",
    "            N_11 += 1\n",
    "        else:\n",
    "            N_01 += 1\n",
    "    for doc in other_corpuses:\n",
    "        if word in doc:\n",
    "            N_10 += 1\n",
    "        else:\n",
    "            N_00 += 1\n",
    "\n",
    "    N = N_11 + N_10 + N_01 + N_00\n",
    "    if N_00 == 0 or N_01 == 0 or N_10 == 0 or N_11 == 0:\n",
    "        return 0, 0\n",
    "\n",
    "    mi = N_11 / N * np.log2(N * N_11 / ((N_10 + N_11) * (N_01 + N_11))) + \\\n",
    "        N_01 / N * np.log2(N * N_01 / ((N_00 + N_01) * (N_01 + N_11))) + \\\n",
    "        N_10 / N * np.log2(N * N_10 / ((N_10 + N_11) * (N_00 + N_10))) + \\\n",
    "        N_00 / N * np.log2(N * N_00 / ((N_00 + N_01) * (N_00 + N_10)))\n",
    "    \n",
    "    chi_squared = N * (N_11 * N_00 - N_10 * N_01) ** 2 / ((N_11 + N_01) * (N_11 + N_10) * (N_10 + N_00) * (N_01 + N_00))\n",
    "    \n",
    "    return mi, chi_squared\n",
    "\n",
    "def top_mutual_information_chi_squared(corpus, other_corpuses):\n",
    "    \"\"\"\n",
    "    Compute the top n words with the highest mutual information in two corpora.\n",
    "    :param corpus1: The first corpus\n",
    "    :param corpus2: The second corpus\n",
    "    :param n: The number of words to return\n",
    "    :return: A df containing the words and their mutual information and chi-squared values\n",
    "    \"\"\"\n",
    "    df = pd.DataFrame(columns=['Word', 'Mutual Information', 'Chi-Squared'])\n",
    "    words = set()\n",
    "    for doc in corpus:\n",
    "        new_words = set(doc)\n",
    "        words = words.union(new_words)\n",
    "    # for doc in other_corpuses:\n",
    "    #     new_words = set(doc)\n",
    "    #     words = words.union(new_words)\n",
    "    print(\"Unique Tokens: \" + str(len(words)))\n",
    "    i = 0\n",
    "    for word in words:\n",
    "        mi, chi_squared = mutual_information_and_chi_squared(word, corpus, other_corpuses)\n",
    "        df.loc[len(df)] = [word, mi, chi_squared]\n",
    "        i += 1\n",
    "        if i % 1000 == 0:\n",
    "            print(str(i) + \" tokens processed\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique Tokens: 3951\n",
      "1000 tokens processed\n",
      "2000 tokens processed\n",
      "3000 tokens processed\n",
      "Unique Tokens: 3348\n",
      "1000 tokens processed\n",
      "2000 tokens processed\n",
      "3000 tokens processed\n"
     ]
    }
   ],
   "source": [
    "# ot_top_mi_chi = top_mutual_information_chi_squared(ot_tokens, nt_tokens+quran_tokens)\n",
    "# ot_top_mi_chi.sort_values(by='Mutual Information', ascending=False, inplace=True)\n",
    "# ot_top_mi_chi.to_csv('ot_mi_x2.csv', index=False)\n",
    "\n",
    "nt_top_mi_chi = top_mutual_information_chi_squared(nt_tokens, ot_tokens+quran_tokens)\n",
    "nt_top_mi_chi.sort_values(by='Mutual Information', ascending=False, inplace=True)\n",
    "nt_top_mi_chi.to_csv('nt_mi_x2.csv', index=False)\n",
    "\n",
    "quran_top_mi_chi = top_mutual_information_chi_squared(quran_tokens, ot_tokens+nt_tokens)\n",
    "quran_top_mi_chi.sort_values(by='Mutual Information', ascending=False, inplace=True)\n",
    "quran_top_mi_chi.to_csv('quran_mi_x2.csv', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Modelling (LDA with Gibbs Sampling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora\n",
    "from gensim.models import LdaModel\n",
    "\n",
    "text = ot_tokens + nt_tokens + quran_tokens\n",
    "\n",
    "dictionary = corpora.Dictionary(text)\n",
    "corpus = [dictionary.doc2bow(doc) for doc in text]\n",
    "\n",
    "lda_model = LdaModel(corpus, num_topics=20, id2word=dictionary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each corpus, compute the average score for each topic by summing the document-topic probability for each document in that corpus and dividing by the total number of documents in the corpus. \n",
    "\n",
    "corpuses = [ot_tokens, nt_tokens, quran_tokens]\n",
    "corpus_names = ['OT', 'NT', 'Quran']\n",
    "corpus_topics = dict()\n",
    "for i in range(len(corpuses)):\n",
    "    corpus = corpuses[i]\n",
    "    corpus_name = corpus_names[i]\n",
    "    corpus_topic = np.zeros(20)\n",
    "    for doc in corpus:\n",
    "        bow = dictionary.doc2bow(doc)\n",
    "        doc_topics = lda_model.get_document_topics(bow)\n",
    "        for topic in doc_topics:\n",
    "            corpus_topic[topic[0]] += topic[1]\n",
    "    corpus_topic /= len(corpus)\n",
    "    corpus_topics[corpus_name] = corpus_topic\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 3 topics for OT\n",
      "Topic 13 : 0.090*\"god\" + 0.063*\"believ\" + 0.042*\"lord\" + 0.031*\"suffer\" + 0.029*\"you\" + 0.027*\"reward\" + 0.026*\"peopl\" + 0.025*\"judgment\" + 0.022*\"them\" + 0.020*\"righteous\"\n",
      "Topic 2 : 0.055*\"god\" + 0.048*\"said\" + 0.036*\"them\" + 0.033*\"i\" + 0.029*\"peopl\" + 0.028*\"say\" + 0.026*\"seek\" + 0.026*\"you\" + 0.025*\"him\" + 0.023*\"lord\"\n",
      "Topic 4 : 0.033*\"other\" + 0.031*\"abraham\" + 0.030*\"deni\" + 0.025*\"measur\" + 0.024*\"children\" + 0.020*\"sea\" + 0.020*\"tribe\" + 0.017*\"disput\" + 0.017*\"inherit\" + 0.016*\"increas\"\n",
      "\n",
      "Top 3 topics for NT\n",
      "Topic 2 : 0.055*\"god\" + 0.048*\"said\" + 0.036*\"them\" + 0.033*\"i\" + 0.029*\"peopl\" + 0.028*\"say\" + 0.026*\"seek\" + 0.026*\"you\" + 0.025*\"him\" + 0.023*\"lord\"\n",
      "Topic 7 : 0.104*\"god\" + 0.051*\"earth\" + 0.050*\"heaven\" + 0.045*\"worship\" + 0.043*\"you\" + 0.024*\"power\" + 0.023*\"them\" + 0.023*\"lord\" + 0.022*\"evil\" + 0.021*\"speak\"\n",
      "Topic 17 : 0.058*\"god\" + 0.046*\"thing\" + 0.033*\"favor\" + 0.031*\"spirit\" + 0.024*\"lord\" + 0.023*\"time\" + 0.022*\"you\" + 0.022*\"thi\" + 0.021*\"warn\" + 0.021*\"them\"\n",
      "\n",
      "Top 3 topics for Quran\n",
      "Topic 13 : 0.090*\"god\" + 0.063*\"believ\" + 0.042*\"lord\" + 0.031*\"suffer\" + 0.029*\"you\" + 0.027*\"reward\" + 0.026*\"peopl\" + 0.025*\"judgment\" + 0.022*\"them\" + 0.020*\"righteous\"\n",
      "Topic 7 : 0.104*\"god\" + 0.051*\"earth\" + 0.050*\"heaven\" + 0.045*\"worship\" + 0.043*\"you\" + 0.024*\"power\" + 0.023*\"them\" + 0.023*\"lord\" + 0.022*\"evil\" + 0.021*\"speak\"\n",
      "Topic 2 : 0.055*\"god\" + 0.048*\"said\" + 0.036*\"them\" + 0.033*\"i\" + 0.029*\"peopl\" + 0.028*\"say\" + 0.026*\"seek\" + 0.026*\"you\" + 0.025*\"him\" + 0.023*\"lord\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# top 3 topics for each corpus\n",
    "for corpus_name in corpus_names:\n",
    "    print(\"Top 3 topics for\", corpus_name)\n",
    "    topics = corpus_topics[corpus_name]\n",
    "    top_topics = np.argsort(topics)[::-1][:3]\n",
    "    for topic in top_topics:\n",
    "        print(\"Topic\", topic, \":\", lda_model.print_topic(topic))\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# twitter_file = \"train.txt\"\n",
    "# twitter = pd.read_csv(twitter_file, sep='\\t', header=None)\n",
    "# twitter.columns = ['ID','Sentiment', 'Text']\n",
    "# twitter = twitter[1:]\n",
    "# twitter.drop(columns=['ID'], inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(data):\n",
    "    \n",
    "    chars_to_remove = re.compile(f'[{string.punctuation}]')\n",
    "    \n",
    "    documents = []\n",
    "    categories = []\n",
    "    \n",
    "    lines = data.split('\\n')\n",
    "    \n",
    "    for line in lines[1:]:\n",
    "        # make a dictionary for each document\n",
    "        # word_id -> count (could also be tf-idf score, etc.)\n",
    "        line = line.strip()\n",
    "        if line:\n",
    "            # split on tabs, we have 3 columns in this tsv format file\n",
    "            tweet_id, category, tweet = line.split('\\t')\n",
    "\n",
    "            # process the words\n",
    "            words = chars_to_remove.sub('',tweet).lower().split()\n",
    "            # add the list of words to the documents list\n",
    "            documents.append(words)\n",
    "            # add the category to the categories list\n",
    "            categories.append(category)\n",
    "            \n",
    "    return documents, categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_file = \"train.txt\"\n",
    "with open(twitter_file, 'r', encoding='utf-8') as f:\n",
    "    data = f.read()\n",
    "documents, categories = preprocess_data(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into training and test sets\n",
    "\n",
    "ratio = 0.8\n",
    "split = int(len(documents) * ratio)\n",
    "train_docs = documents[:split]\n",
    "train_cat = categories[:split]\n",
    "test_docs = documents[split:]\n",
    "test_cats = categories[split:]\n",
    "\n",
    "train_vocab = set()\n",
    "for doc in train_docs:\n",
    "    for word in doc:\n",
    "        train_vocab.add(word)\n",
    "\n",
    "test_vocab = set()\n",
    "for doc in test_docs:\n",
    "    for word in doc:\n",
    "        test_vocab.add(word)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2id = {}\n",
    "for word_id,word in enumerate(train_vocab):\n",
    "    word2id[word] = word_id\n",
    "    \n",
    "cat2id = {}\n",
    "for cat_id,cat in enumerate(set(train_cat)):\n",
    "    cat2id[cat] = cat_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build a BOW representation of the files: use the scipy \n",
    "# data is the preprocessed_data\n",
    "# word2id maps words to their ids\n",
    "def convert_to_bow_matrix(preprocessed_data, word2id):\n",
    "    \n",
    "    # matrix size is number of docs x vocab size + 1 (for OOV)\n",
    "    matrix_size = (len(preprocessed_data),len(word2id)+1)\n",
    "    oov_index = len(word2id)\n",
    "    # matrix indexed by [doc_id, token_id]\n",
    "    X = scipy.sparse.dok_matrix(matrix_size)\n",
    "\n",
    "    # iterate through all documents in the dataset\n",
    "    for doc_id,doc in enumerate(preprocessed_data):\n",
    "        for word in doc:\n",
    "            # default is 0, so just add to the count for this word in this doc\n",
    "            # if the word is oov, increment the oov_index\n",
    "            X[doc_id,word2id.get(word,oov_index)] += 1\n",
    "    \n",
    "    return X\n",
    "\n",
    "X_train = convert_to_bow_matrix(train_docs, word2id)\n",
    "y_train = [cat2id[cat] for cat in train_cat]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_test_data(file, ratio):\n",
    "    with open(file, 'r', encoding='utf-8') as f:\n",
    "        data = f.read()\n",
    "    documents, categories = preprocess_data(data)\n",
    "    split = int(len(documents) * ratio)\n",
    "    train_docs = documents[:split]\n",
    "    train_cat = categories[:split]\n",
    "    test_docs = documents[split:]\n",
    "    test_cats = categories[split:]\n",
    "\n",
    "    train_vocab = set()\n",
    "    for doc in train_docs:\n",
    "        for word in doc:\n",
    "            train_vocab.add(word)\n",
    "\n",
    "    test_vocab = set()\n",
    "    for doc in test_docs:\n",
    "        for word in doc:\n",
    "            test_vocab.add(word)\n",
    "\n",
    "    word2id = {}\n",
    "    for word_id,word in enumerate(train_vocab):\n",
    "        word2id[word] = word_id\n",
    "    \n",
    "    cat2id = {}\n",
    "    for cat_id,cat in enumerate(set(train_cat)):\n",
    "        cat2id[cat] = cat_id\n",
    "\n",
    "    X_train = convert_to_bow_matrix(train_docs, word2id)\n",
    "    y_train = [cat2id[cat] for cat in train_cat]\n",
    "\n",
    "    X_test = convert_to_bow_matrix(test_docs, word2id)\n",
    "    y_test = [cat2id[cat] for cat in test_cats]\n",
    "\n",
    "    return X_train, y_train, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = sklearn.svm.SVC(C=1000, kernel =\"linear\")\n",
    "# # then train the model!\n",
    "# model.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_train_predictions = model.predict(X_train)\n",
    "\n",
    "# now can compute any metrics we care about. Let's quickly do accuracy\n",
    "def compute_accuracy(predictions, true_values):\n",
    "    num_correct = 0\n",
    "    num_total = len(predictions)\n",
    "    for predicted,true in zip(predictions,true_values):\n",
    "        if predicted==true:\n",
    "            num_correct += 1\n",
    "    return num_correct / num_total\n",
    "\n",
    "# accuracy = compute_accuracy(y_train_predictions,y_train)\n",
    "# print(\"Accuracy:\",accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# y_test_predictions = model.predict(X_test)\n",
    "# cat_names = []\n",
    "# for cat,cid in sorted(cat2id.items(),key=lambda x:x[1]):\n",
    "#     cat_names.append(cat)\n",
    "# print(classification_report(y_test, y_test_predictions, target_names=cat_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = sklearn.ensemble.RandomForestClassifier()\n",
    "# model.fit(X_train,y_train)\n",
    "\n",
    "# y_train_predictions = model.predict(X_train)\n",
    "# print(\"Train accuracy was:\",compute_accuracy(y_train_predictions,y_train))\n",
    "# y_test_predictions = model.predict(X_test)\n",
    "# print(\"Test accuracy was:\",compute_accuracy(y_test_predictions,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_accuracy(model, X_train, y_train, X_test, y_test):\n",
    "    model.fit(X_train,y_train)\n",
    "    y_train_predictions = model.predict(X_train)\n",
    "    train_accuracy = compute_accuracy(y_train_predictions,y_train)\n",
    "    y_test_predictions = model.predict(X_test)\n",
    "    test_accuracy = compute_accuracy(y_test_predictions,y_test)\n",
    "    return train_accuracy, test_accuracy\n",
    "\n",
    "# model_list = [sklearn.svm.SVC(C=1000, kernel =\"linear\"), sklearn.ensemble.RandomForestClassifier(), sklearn.ensemble.GradientBoostingClassifier(), sklearn.linear_model.LogisticRegression(), sklearn.naive_bayes.MultinomialNB(), sklearn.naive_bayes.BernoulliNB(), sklearn.neighbors.KNeighborsClassifier(), sklearn.tree.DecisionTreeClassifier(), sklearn.neural_network.MLPClassifier()]\n",
    "# model_names = ['SVM', 'Random Forest', 'Gradient Boosting', 'Logistic Regression', 'Multinomial Naive Bayes', 'Bernoulli Naive Bayes', 'K-Nearest Neighbors', 'Decision Tree', 'MLP']\n",
    "\n",
    "# train_accuracies = []\n",
    "# test_accuracies = []\n",
    "\n",
    "# for model in model_list:\n",
    "#     train_accuracy, test_accuracy = get_model_accuracy(model, X_train, y_train, X_test, y_test)\n",
    "#     train_accuracies.append(train_accuracy)\n",
    "#     test_accuracies.append(test_accuracy)\n",
    "#     print(\"Model:\", model)\n",
    "#     print(\"Train accuracy was:\", train_accuracy)\n",
    "#     print(\"Test accuracy was:\", test_accuracy)\n",
    "#     print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: LogisticRegression(C=0.001)\n",
      "Train accuracy was: 0.5226744532507002\n",
      "Test accuracy was: 0.500804289544236\n",
      "\n",
      "Model: LogisticRegression(C=0.01)\n",
      "Train accuracy was: 0.6384005720755617\n",
      "Test accuracy was: 0.5715817694369973\n",
      "\n",
      "Model: LogisticRegression(C=0.1)\n",
      "Train accuracy was: 0.8103211965913831\n",
      "Test accuracy was: 0.6300268096514745\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\adamg\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: LogisticRegression(C=1)\n",
      "Train accuracy was: 0.9737202788868363\n",
      "Test accuracy was: 0.6187667560321716\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\adamg\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: LogisticRegression(C=10)\n",
      "Train accuracy was: 0.9966033013527203\n",
      "Test accuracy was: 0.5989276139410188\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\adamg\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: LogisticRegression(C=100)\n",
      "Train accuracy was: 0.9991657231392647\n",
      "Test accuracy was: 0.5903485254691689\n",
      "\n",
      "Model: LogisticRegression(C=1000)\n",
      "Train accuracy was: 0.9994040879566176\n",
      "Test accuracy was: 0.5898123324396782\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Logistic regression parameter sweep\n",
    "\n",
    "X_train, y_train, X_test, y_test = get_train_test_data(\"train.txt\", 0.9)\n",
    "\n",
    "C_values = [0.001, 0.01, 0.1, 1, 10, 100, 1000]\n",
    "train_accuracies = []\n",
    "test_accuracies = []\n",
    "\n",
    "for C in C_values:\n",
    "    model = sklearn.linear_model.LogisticRegression(C=C)\n",
    "    train_accuracy, test_accuracy = get_model_accuracy(model, X_train, y_train, X_test, y_test)\n",
    "    train_accuracies.append(train_accuracy)\n",
    "    test_accuracies.append(test_accuracy)\n",
    "    print(\"Model:\", model)\n",
    "    print(\"Train accuracy was:\", train_accuracy)\n",
    "    print(\"Test accuracy was:\", test_accuracy)\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
